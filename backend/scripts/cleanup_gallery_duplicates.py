#!/usr/bin/env python3
"""
æ¸…ç†å›¾åº“é‡å¤ç´ æ
åŸºäºå›¾ç‰‡å†…å®¹å“ˆå¸Œè¯†åˆ«å¹¶åˆ é™¤é‡å¤å›¾ç‰‡
"""
import hashlib
import json
import sys
from pathlib import Path
from collections import defaultdict
from PIL import Image

try:
    import imagehash
    HAS_IMAGEHASH = True
except ImportError:
    HAS_IMAGEHASH = False

# æ·»åŠ  backend ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))


def get_image_hash(image_path: Path) -> str:
    """è®¡ç®—å›¾ç‰‡çš„æ„ŸçŸ¥å“ˆå¸Œï¼ˆå¯è¯†åˆ«ç›¸ä¼¼å›¾ç‰‡ï¼‰"""
    if not HAS_IMAGEHASH:
        return None

    try:
        img = Image.open(image_path)
        # ä½¿ç”¨å¹³å‡å“ˆå¸Œï¼Œå¯¹è½»å¾®å˜åŒ–ä¸æ•æ„Ÿ
        return str(imagehash.average_hash(img, hash_size=8))
    except Exception as e:
        print(f"  âš ï¸  æ— æ³•è®¡ç®—å“ˆå¸Œ: {image_path.name} - {e}")
        return None


def get_file_hash(image_path: Path) -> str:
    """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œï¼ˆç²¾ç¡®åŒ¹é…ï¼‰"""
    try:
        with open(image_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"  âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶: {image_path.name} - {e}")
        return None


def analyze_duplicates():
    """åˆ†æå›¾åº“ä¸­çš„é‡å¤å›¾ç‰‡"""
    gallery_dir = Path(__file__).parent.parent / "data" / "gallery"
    images_dir = gallery_dir / "images"
    metadata_file = gallery_dir / "metadata.json"

    if not images_dir.exists():
        print("âŒ å›¾åº“ç›®å½•ä¸å­˜åœ¨")
        return

    # åŠ è½½å…ƒæ•°æ®
    if metadata_file.exists():
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
    else:
        print("âš ï¸  å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        metadata = {"items": []}

    print("\n" + "="*60)
    print("ğŸ“Š å›¾åº“é‡å¤åˆ†æ")
    print("="*60)

    # è·å–æ‰€æœ‰å›¾ç‰‡
    image_files = list(images_dir.glob("*.jpg"))
    print(f"\nğŸ“ å›¾åº“ä¸­å…±æœ‰ {len(image_files)} å¼ å›¾ç‰‡")
    print(f"ğŸ“„ å…ƒæ•°æ®ä¸­è®°å½• {len(metadata['items'])} æ¡")

    # æ£€æŸ¥ç²¾ç¡®é‡å¤ï¼ˆåŸºäºæ–‡ä»¶å“ˆå¸Œï¼‰
    print("\nğŸ” æ£€æŸ¥ç²¾ç¡®é‡å¤ï¼ˆç›¸åŒæ–‡ä»¶ï¼‰...")
    file_hashes = defaultdict(list)

    for img_path in image_files:
        file_hash = get_file_hash(img_path)
        if file_hash:
            file_hashes[file_hash].append(img_path)

    exact_duplicates = {h: files for h, files in file_hashes.items() if len(files) > 1}

    if exact_duplicates:
        print(f"\nâš ï¸  å‘ç° {len(exact_duplicates)} ç»„ç²¾ç¡®é‡å¤:")
        for hash_val, files in exact_duplicates.items():
            print(f"\n  å“ˆå¸Œ: {hash_val[:16]}...")
            for f in files:
                print(f"    - {f.name}")
    else:
        print("âœ… æ²¡æœ‰å‘ç°ç²¾ç¡®é‡å¤çš„æ–‡ä»¶")

    # æ£€æŸ¥ç›¸ä¼¼å›¾ç‰‡ï¼ˆåŸºäºæ„ŸçŸ¥å“ˆå¸Œï¼‰
    print("\nğŸ” æ£€æŸ¥ç›¸ä¼¼å›¾ç‰‡ï¼ˆè§†è§‰ç›¸ä¼¼ï¼‰...")

    try:
        image_hashes = {}
        for img_path in image_files:
            img_hash = get_image_hash(img_path)
            if img_hash:
                if img_hash not in image_hashes:
                    image_hashes[img_hash] = []
                image_hashes[img_hash].append(img_path)

        similar_groups = {h: files for h, files in image_hashes.items() if len(files) > 1}

        if similar_groups:
            print(f"\nâš ï¸  å‘ç° {len(similar_groups)} ç»„ç›¸ä¼¼å›¾ç‰‡:")
            for hash_val, files in similar_groups.items():
                print(f"\n  æ„ŸçŸ¥å“ˆå¸Œ: {hash_val}")
                for f in files:
                    size_kb = f.stat().st_size / 1024
                    print(f"    - {f.name} ({size_kb:.1f} KB)")
        else:
            print("âœ… æ²¡æœ‰å‘ç°è§†è§‰ç›¸ä¼¼çš„å›¾ç‰‡")

    except ImportError:
        print("âš ï¸  æœªå®‰è£… imagehash åº“ï¼Œè·³è¿‡ç›¸ä¼¼åº¦æ£€æŸ¥")
        print("   å¯è¿è¡Œ: pip install imagehash")

    # æ£€æŸ¥å…ƒæ•°æ®é‡å¤
    print("\nğŸ” æ£€æŸ¥å…ƒæ•°æ®é‡å¤...")
    analysis_signatures = defaultdict(list)

    for item in metadata['items']:
        analysis = item.get('analysis', {})
        style = analysis.get('style', {})

        # åˆ›å»ºç­¾åï¼šé£æ ¼æ ‡ç­¾ + ä¸»è¦å…ƒç´ 
        tags = tuple(sorted(style.get('tags', [])))
        primary = tuple(sorted([
            e.get('type', '')
            for e in analysis.get('elements', {}).get('primary', [])
        ]))

        signature = (tags, primary)
        analysis_signatures[signature].append(item['id'])

    duplicate_analyses = {
        sig: ids for sig, ids in analysis_signatures.items()
        if len(ids) > 1
    }

    if duplicate_analyses:
        print(f"\nâš ï¸  å‘ç° {len(duplicate_analyses)} ç»„å…ƒæ•°æ®é‡å¤:")
        for (tags, elements), ids in duplicate_analyses.items():
            print(f"\n  æ ‡ç­¾: {', '.join(tags)}")
            print(f"  å…ƒç´ : {', '.join(elements)}")
            print(f"  æ•°é‡: {len(ids)} æ¡è®°å½•")
    else:
        print("âœ… æ²¡æœ‰å‘ç°é‡å¤çš„å…ƒæ•°æ®")

    # ç»Ÿè®¡å»ºè®®
    print("\n" + "="*60)
    print("ğŸ’¡ æ¸…ç†å»ºè®®")
    print("="*60)

    total_duplicates = sum(len(files) - 1 for files in exact_duplicates.values())
    if similar_groups:
        total_duplicates += sum(len(files) - 1 for files in similar_groups.values())

    if total_duplicates > 0:
        print(f"\nå¯åˆ é™¤çº¦ {total_duplicates} å¼ é‡å¤å›¾ç‰‡")
        print(f"æ¸…ç†åå‰©ä½™çº¦ {len(image_files) - total_duplicates} å¼ å”¯ä¸€å›¾ç‰‡")
    else:
        print("\nå›¾åº“å·²æ¸…ç†ï¼Œæ— é‡å¤å›¾ç‰‡")

    return {
        'total': len(image_files),
        'exact_duplicates': exact_duplicates,
        'similar_groups': similar_groups if 'similar_groups' in locals() else {},
        'duplicate_analyses': duplicate_analyses
    }


def cleanup_duplicates(dry_run=True):
    """æ¸…ç†é‡å¤å›¾ç‰‡"""
    result = analyze_duplicates()

    if not result:
        return

    exact_duplicates = result['exact_duplicates']
    similar_groups = result['similar_groups']

    if not exact_duplicates and not similar_groups:
        print("\nâœ… æ— éœ€æ¸…ç†")
        return

    print("\n" + "="*60)
    print("ğŸ—‘ï¸  å¼€å§‹æ¸…ç†")
    print("="*60)

    gallery_dir = Path(__file__).parent.parent / "data" / "gallery"
    images_dir = gallery_dir / "images"
    embeddings_dir = gallery_dir / "embeddings"
    metadata_file = gallery_dir / "metadata.json"

    # åŠ è½½å…ƒæ•°æ®
    with open(metadata_file, 'r', encoding='utf-8') as f:
        metadata = json.load(f)

    files_to_delete = []
    ids_to_remove = set()

    # æ”¶é›†è¦åˆ é™¤çš„æ–‡ä»¶ï¼ˆæ¯ç»„ä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
    for files in exact_duplicates.values():
        files_to_delete.extend(files[1:])  # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä½™

    for files in similar_groups.values():
        files_to_delete.extend(files[1:])  # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä½™

    # ä»æ–‡ä»¶åæå– ID
    for file_path in files_to_delete:
        file_id = file_path.stem
        ids_to_remove.add(file_id)

    if dry_run:
        print(f"\nâš ï¸  æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼ï¼ˆä¸ä¼šå®é™…åˆ é™¤ï¼‰")
        print(f"å°†åˆ é™¤ {len(files_to_delete)} ä¸ªæ–‡ä»¶:")
        for f in files_to_delete[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {f.name}")
        if len(files_to_delete) > 10:
            print(f"  ... è¿˜æœ‰ {len(files_to_delete) - 10} ä¸ª")

        print(f"\nå°†ä»å…ƒæ•°æ®ä¸­åˆ é™¤ {len(ids_to_remove)} æ¡è®°å½•")

        print("\nè¦æ‰§è¡Œå®é™…åˆ é™¤ï¼Œè¯·è¿è¡Œ:")
        print("  python cleanup_gallery_duplicates.py --execute")
    else:
        # å®é™…åˆ é™¤
        deleted_count = 0

        for file_path in files_to_delete:
            try:
                # åˆ é™¤å›¾ç‰‡æ–‡ä»¶
                if file_path.exists():
                    file_path.unlink()
                    print(f"  âœ“ åˆ é™¤å›¾ç‰‡: {file_path.name}")

                # åˆ é™¤å¯¹åº”çš„åµŒå…¥å‘é‡
                file_id = file_path.stem
                embedding_path = embeddings_dir / f"{file_id}.npy"
                if embedding_path.exists():
                    embedding_path.unlink()
                    print(f"  âœ“ åˆ é™¤å‘é‡: {embedding_path.name}")

                deleted_count += 1
            except Exception as e:
                print(f"  âŒ åˆ é™¤å¤±è´¥: {file_path.name} - {e}")

        # æ›´æ–°å…ƒæ•°æ®
        original_count = len(metadata['items'])
        metadata['items'] = [
            item for item in metadata['items']
            if item['id'] not in ids_to_remove
        ]

        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… æ¸…ç†å®Œæˆ:")
        print(f"  åˆ é™¤æ–‡ä»¶: {deleted_count}")
        print(f"  åˆ é™¤è®°å½•: {original_count - len(metadata['items'])}")
        print(f"  å‰©ä½™å›¾ç‰‡: {len(metadata['items'])}")


if __name__ == "__main__":
    import sys

    if "--execute" in sys.argv:
        cleanup_duplicates(dry_run=False)
    else:
        analyze_duplicates()
